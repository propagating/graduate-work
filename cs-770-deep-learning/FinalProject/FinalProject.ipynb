{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a GAN to Generate Art Styled After Yoshitaka Amano\n",
    "\n",
    "## Ryan Richardson\n",
    "## CS 770 - Deep Learning\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The task I choose to pursue was the generation of images in the style of Yoshitaka Amano using deep learning. Various deep learning models can be employed for this task, each with its own strengths and performance characteristics. The category of models I chose to pursue was Generative Adversarial Networks (GANs), which are widely used.  Specifically I would compare a normal GAN to a StyleGan for image generation. I would also attempted to augment my networks with an Augmented Discriminator in an attempt to compensate for a low amount of training data. When comparing the performance of these models, factors such as image quality, diversity, realism, and control over attributes need to be considered.\n",
    "\n",
    "An ADA (Augmented Data Augmentation) technique can be used to augment small training sets for a Generative Adversarial Network (GAN). When the available training data is limited, ADA provides a way to artificially increase the dataset size and improve the training process. ADA achieves this by applying various data augmentation techniques to the existing training samples, creating additional synthetic data points. These augmentation techniques can include transformations such as rotations, translations, scaling, flipping, and adding random noise. By introducing these augmented samples during training, the GAN learns from a more diverse and varied set of data, helping to improve its generalization and ability to generate realistic and diverse images. ADA is particularly useful when the original training set is small, as it helps mitigate overfitting and improves the model's robustness by providing a broader range of training examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data set consists of scanned images of Yoshitaka Amano's art as well as images obtained through web scraping from online sources that feature his artwork. Yoshitaka Amano is a renowned Japanese artist known for his distinctive style, particularly his illustrations for various media, including manga, anime, and video games. The scanned images in the data set are obtained from physical copies of Amano's books, ensuring a high-quality representations of his original creations. Additionally, images scraped from online sources provide a broader collection. Despite this, I was only able to build a training set of ~1800 images, which necessitated the use of augmentation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To achieve the results, both a GAN and a StyleGAN were trained on a small dataset, with the GAN performing worse than the StyleGAN. To improve their performance, an ADA (Augmented Data Augmentation) technique was employed, which artificially expanded the dataset by applying various data augmentation techniques. Different hyperparameters were tuned for both models, including learning rate, batch size, and network architecture, in an attempt to enhance their performance. Additionally, different augmentations were explored within the ADA, such as rotations, translations, scaling, flipping, and adding random noise. The models were also trained on different image sizes to evaluate their impact on the generated results. However, due to excessive training times, there was a limit on the number of variations that could be tried.\n",
    "\n",
    "Despite the limitations, the StyleGAN exhibited superior performance compared to the GAN. Some images generated by the StyleGAN successfully emulated the line work and color details of the target artist. This suggests that the StyleGAN's architecture, which incorporates style-based techniques and provides fine-grained control over image attributes, better captured the nuances of the artist's style. Although the GAN's performance was subpar, the attempts at hyperparameter tuning and augmentations through ADA helped mitigate the limitations of the small dataset, providing some improvements. However, the training times and the limited number of variations that could be explored posed challenges in fully optimizing the models' performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conlusion + Future Improvements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In conclusion, when working with a small dataset, the StyleGAN outperformed the GAN in generating images that closely resembled the target artist's line work and color detail. The application of ADA and the exploration of different hyperparameters and augmentations helped enhance the models' performance, albeit with limitations due to training times and the number of variations that could be tested. Despite these challenges, the StyleGAN demonstrated its effectiveness in capturing the artist's style.\n",
    "\n",
    "To further improve the method, several enhancements can be considered. Firstly, transfer learning can be utilized by pretraining the models on larger image collections that exhibit similar artistic styles. This approach leverages the knowledge learned from these larger datasets to bootstrap the training process on the small dataset, potentially leading to better results. Additionally, acquiring a more comprehensive and diverse image collection specifically curated for the target artist could provide richer training data, leading to improved performance.\n",
    "\n",
    "Automated hyperparameter tuning techniques, such as Bayesian optimization or evolutionary algorithms, can be employed to efficiently search for optimal hyperparameter configurations. This approach can save significant time and effort by automatically exploring a wide range of hyperparameter settings to find the best combination for the models.\n",
    "\n",
    "Moreover, by utilizing multiple workstations or distributed computing resources, the models can be trained in parallel, allowing for quicker experimentation and evaluation of different variations. This approach reduces the overall training time and enables testing a larger number of hyperparameter combinations, augmentations, and architectural variations, leading to more comprehensive and refined models.\n",
    "\n",
    "In summary, improvements to the method include transfer learning, better image collections, automated hyperparameter tuning, and parallel training using multiple workstations. These enhancements have the potential to further enhance the performance of the models in generating artwork that closely resembles the desired style and details of the target artist.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from torchvision.transforms import InterpolationMode"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using GAN To Generate Images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataroot = \"data\"\n",
    "workers = 2\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "nc = 3\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "num_epochs = 500\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ngpu = 1"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Training Set and Create DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.RandomResizedCrop(image_size, scale=(0.9, 1.1), ratio=(0.9, 1.1), interpolation=InterpolationMode.BICUBIC),\n",
    "                               transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.RandomVerticalFlip(),\n",
    "                               transforms.RandAugment(),\n",
    "                               transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Generator and Discriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "#  to ``mean=0``, ``stdev=0.02``.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 64 x 64``\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "# like this: ``to mean=0, stdev=0.2``.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Style GAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\.conda\\envs\\PhD\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "import gdown\n",
    "from zipfile import ZipFile\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:13.930351200Z",
     "start_time": "2023-05-12T19:30:58.485244600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1848 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def log2(x):\n",
    "    return int(np.log2(x))\n",
    "\n",
    "\n",
    "# we use different batch size for different resolution, so larger image size\n",
    "# could fit into GPU memory. The keys is image resolution in log2\n",
    "batch_sizes = {2: 16, 3: 16, 4: 16, 5: 16, 6: 16, 7: 8, 8: 4, 9: 2, 10: 1}\n",
    "# We adjust the train step accordingly\n",
    "train_step_ratio = {k: batch_sizes[2] / v for k, v in batch_sizes.items()}\n",
    "\n",
    "# Create a dataset from our folder, and rescale the images to the [0-1] range:\n",
    "\n",
    "ds_train = keras.utils.image_dataset_from_directory(\n",
    "    \"data/training\", label_mode=None, image_size=(64, 64), batch_size=32\n",
    ")\n",
    "ds_train = ds_train.map(lambda x: x / 255.0)\n",
    "\n",
    "\n",
    "def resize_image(res, image):\n",
    "    # only donwsampling, so use nearest neighbor that is faster to run\n",
    "    image = tf.image.resize(\n",
    "        image, (res, res), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_dataloader(res):\n",
    "    batch_size = batch_sizes[log2(res)]\n",
    "    # NOTE: we unbatch the dataset so we can `batch()` it again with the `drop_remainder=True` option\n",
    "    # since the model only supports a single batch size\n",
    "    dl = ds_train.map(partial(resize_image, res), num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "    dl = dl.shuffle(200).batch(batch_size, drop_remainder=True).prefetch(1).repeat()\n",
    "    return dl\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:14.565351500Z",
     "start_time": "2023-05-12T19:31:13.939351300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def plot_images(images, log2_res, fname=\"\"):\n",
    "    scales = {2: 0.5, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8}\n",
    "    scale = scales[log2_res]\n",
    "\n",
    "    grid_col = min(images.shape[0], int(32 // scale))\n",
    "    grid_row = 1\n",
    "\n",
    "    f, axarr = plt.subplots(\n",
    "        grid_row, grid_col, figsize=(grid_col * scale, grid_row * scale)\n",
    "    )\n",
    "\n",
    "    for row in range(grid_row):\n",
    "        ax = axarr if grid_row == 1 else axarr[row]\n",
    "        for col in range(grid_col):\n",
    "            ax[col].imshow(images[row * grid_col + col])\n",
    "            ax[col].axis(\"off\")\n",
    "    plt.show()\n",
    "    if fname:\n",
    "        f.savefig(fname)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:14.580349600Z",
     "start_time": "2023-05-12T19:31:14.572350Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Custom Layers and Transformations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def fade_in(alpha, a, b):\n",
    "    return alpha * a + (1.0 - alpha) * b\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def pixel_norm(x, epsilon=1e-8):\n",
    "    return x / tf.math.sqrt(tf.reduce_mean(x ** 2, axis=-1, keepdims=True) + epsilon)\n",
    "\n",
    "\n",
    "def minibatch_std(input_tensor, epsilon=1e-8):\n",
    "    n, h, w, c = tf.shape(input_tensor)\n",
    "    group_size = tf.minimum(4, n)\n",
    "    x = tf.reshape(input_tensor, [group_size, -1, h, w, c])\n",
    "    group_mean, group_var = tf.nn.moments(x, axes=(0), keepdims=False)\n",
    "    group_std = tf.sqrt(group_var + epsilon)\n",
    "    avg_std = tf.reduce_mean(group_std, axis=[1, 2, 3], keepdims=True)\n",
    "    x = tf.tile(avg_std, [group_size, h, w, 1])\n",
    "    return tf.concat([input_tensor, x], axis=-1)\n",
    "\n",
    "\n",
    "class EqualizedConv(layers.Layer):\n",
    "    def __init__(self, out_channels, kernel=3, gain=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernel = kernel\n",
    "        self.out_channels = out_channels\n",
    "        self.gain = gain\n",
    "        self.pad = kernel != 1\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_channels = input_shape[-1]\n",
    "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
    "        self.w = self.add_weight(\n",
    "            shape=[self.kernel, self.kernel, self.in_channels, self.out_channels],\n",
    "            initializer=initializer,\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.out_channels,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
    "        )\n",
    "        fan_in = self.kernel * self.kernel * self.in_channels\n",
    "        self.scale = tf.sqrt(self.gain / fan_in)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.pad:\n",
    "            x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"REFLECT\")\n",
    "        else:\n",
    "            x = inputs\n",
    "        output = (\n",
    "            tf.nn.conv2d(x, self.scale * self.w, strides=1, padding=\"VALID\") + self.b\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "class EqualizedDense(layers.Layer):\n",
    "    def __init__(self, units, gain=2, learning_rate_multiplier=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gain = gain\n",
    "        self.learning_rate_multiplier = learning_rate_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_channels = input_shape[-1]\n",
    "        initializer = keras.initializers.RandomNormal(\n",
    "            mean=0.0, stddev=1.0 / self.learning_rate_multiplier\n",
    "        )\n",
    "        self.w = self.add_weight(\n",
    "            shape=[self.in_channels, self.units],\n",
    "            initializer=initializer,\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
    "        )\n",
    "        fan_in = self.in_channels\n",
    "        self.scale = tf.sqrt(self.gain / fan_in)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = tf.add(tf.matmul(inputs, self.scale * self.w), self.b)\n",
    "        return output * self.learning_rate_multiplier\n",
    "\n",
    "\n",
    "class AddNoise(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        n, h, w, c = input_shape[0]\n",
    "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
    "        self.b = self.add_weight(\n",
    "            shape=[1, 1, 1, c], initializer=initializer, trainable=True, name=\"kernel\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, noise = inputs\n",
    "        output = x + self.b * noise\n",
    "        return output\n",
    "\n",
    "\n",
    "class AdaIN(layers.Layer):\n",
    "    def __init__(self, gain=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gain = gain\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        x_shape = input_shapes[0]\n",
    "        w_shape = input_shapes[1]\n",
    "\n",
    "        self.w_channels = w_shape[-1]\n",
    "        self.x_channels = x_shape[-1]\n",
    "\n",
    "        self.dense_1 = EqualizedDense(self.x_channels, gain=1)\n",
    "        self.dense_2 = EqualizedDense(self.x_channels, gain=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, w = inputs\n",
    "        ys = tf.reshape(self.dense_1(w), (-1, 1, 1, self.x_channels))\n",
    "        yb = tf.reshape(self.dense_2(w), (-1, 1, 1, self.x_channels))\n",
    "        return ys * x + yb\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:14.614850100Z",
     "start_time": "2023-05-12T19:31:14.594851200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the Generator, Discriminator, and a Noise Map Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def Mapping(num_stages, input_shape=512):\n",
    "    z = layers.Input(shape=(input_shape))\n",
    "    w = pixel_norm(z)\n",
    "    for i in range(8):\n",
    "        w = EqualizedDense(512, learning_rate_multiplier=0.01)(w)\n",
    "        w = layers.LeakyReLU(0.2)(w)\n",
    "    w = tf.tile(tf.expand_dims(w, 1), (1, num_stages, 1))\n",
    "    return keras.Model(z, w, name=\"mapping\")\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, start_res_log2, target_res_log2):\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
    "        # list of generator blocks at increasing resolution\n",
    "        self.g_blocks = []\n",
    "        # list of layers to convert g_block activation to RGB\n",
    "        self.to_rgb = []\n",
    "        # list of noise input of different resolutions into g_blocks\n",
    "        self.noise_inputs = []\n",
    "        # filter size to use at each stage, keys are log2(resolution)\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }  # 1024x1024\n",
    "\n",
    "        start_res = 2 ** start_res_log2\n",
    "        self.input_shape = (start_res, start_res, self.filter_nums[start_res_log2])\n",
    "        self.g_input = layers.Input(self.input_shape, name=\"generator_input\")\n",
    "\n",
    "        for i in range(start_res_log2, target_res_log2 + 1):\n",
    "            filter_num = self.filter_nums[i]\n",
    "            res = 2 ** i\n",
    "            self.noise_inputs.append(\n",
    "                layers.Input(shape=(res, res, 1), name=f\"noise_{res}x{res}\")\n",
    "            )\n",
    "            to_rgb = Sequential(\n",
    "                [\n",
    "                    layers.InputLayer(input_shape=(res, res, filter_num)),\n",
    "                    EqualizedConv(3, 1, gain=1),\n",
    "                ],\n",
    "                name=f\"to_rgb_{res}x{res}\",\n",
    "            )\n",
    "            self.to_rgb.append(to_rgb)\n",
    "            is_base = i == self.start_res_log2\n",
    "            if is_base:\n",
    "                input_shape = (res, res, self.filter_nums[i - 1])\n",
    "            else:\n",
    "                input_shape = (2 ** (i - 1), 2 ** (i - 1), self.filter_nums[i - 1])\n",
    "            g_block = self.build_block(\n",
    "                filter_num, res=res, input_shape=input_shape, is_base=is_base\n",
    "            )\n",
    "            self.g_blocks.append(g_block)\n",
    "\n",
    "    def build_block(self, filter_num, res, input_shape, is_base):\n",
    "        input_tensor = layers.Input(shape=input_shape, name=f\"g_{res}\")\n",
    "        noise = layers.Input(shape=(res, res, 1), name=f\"noise_{res}\")\n",
    "        w = layers.Input(shape=512)\n",
    "        x = input_tensor\n",
    "\n",
    "        if not is_base:\n",
    "            x = layers.UpSampling2D((2, 2))(x)\n",
    "            x = EqualizedConv(filter_num, 3)(x)\n",
    "\n",
    "        x = AddNoise()([x, noise])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = AdaIN()([x, w])\n",
    "\n",
    "        x = EqualizedConv(filter_num, 3)(x)\n",
    "        x = AddNoise()([x, noise])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = AdaIN()([x, w])\n",
    "        return keras.Model([input_tensor, w, noise], x, name=f\"genblock_{res}x{res}\")\n",
    "\n",
    "    def grow(self, res_log2):\n",
    "        res = 2 ** res_log2\n",
    "\n",
    "        num_stages = res_log2 - self.start_res_log2 + 1\n",
    "        w = layers.Input(shape=(self.num_stages, 512), name=\"w\")\n",
    "\n",
    "        alpha = layers.Input(shape=(1), name=\"g_alpha\")\n",
    "        x = self.g_blocks[0]([self.g_input, w[:, 0], self.noise_inputs[0]])\n",
    "\n",
    "        if num_stages == 1:\n",
    "            rgb = self.to_rgb[0](x)\n",
    "        else:\n",
    "            for i in range(1, num_stages - 1):\n",
    "\n",
    "                x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n",
    "\n",
    "            old_rgb = self.to_rgb[num_stages - 2](x)\n",
    "            old_rgb = layers.UpSampling2D((2, 2))(old_rgb)\n",
    "\n",
    "            i = num_stages - 1\n",
    "            x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n",
    "\n",
    "            new_rgb = self.to_rgb[i](x)\n",
    "\n",
    "            rgb = fade_in(alpha[0], new_rgb, old_rgb)\n",
    "\n",
    "        return keras.Model(\n",
    "            [self.g_input, w, self.noise_inputs, alpha],\n",
    "            rgb,\n",
    "            name=f\"generator_{res}_x_{res}\",\n",
    "        )\n",
    "\n",
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, start_res_log2, target_res_log2):\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
    "        # filter size to use at each stage, keys are log2(resolution)\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }  # 1024x1024\n",
    "        # list of discriminator blocks at increasing resolution\n",
    "        self.d_blocks = []\n",
    "        # list of layers to convert RGB into activation for d_blocks inputs\n",
    "        self.from_rgb = []\n",
    "\n",
    "        for res_log2 in range(self.start_res_log2, self.target_res_log2 + 1):\n",
    "            res = 2 ** res_log2\n",
    "            filter_num = self.filter_nums[res_log2]\n",
    "            from_rgb = Sequential(\n",
    "                [\n",
    "                    layers.InputLayer(\n",
    "                        input_shape=(res, res, 3), name=f\"from_rgb_input_{res}\"\n",
    "                    ),\n",
    "                    EqualizedConv(filter_num, 1),\n",
    "                    layers.LeakyReLU(0.2),\n",
    "                ],\n",
    "                name=f\"from_rgb_{res}\",\n",
    "            )\n",
    "\n",
    "            self.from_rgb.append(from_rgb)\n",
    "\n",
    "            input_shape = (res, res, filter_num)\n",
    "            if len(self.d_blocks) == 0:\n",
    "                d_block = self.build_base(filter_num, res)\n",
    "            else:\n",
    "                d_block = self.build_block(\n",
    "                    filter_num, self.filter_nums[res_log2 - 1], res\n",
    "                )\n",
    "\n",
    "            self.d_blocks.append(d_block)\n",
    "\n",
    "    def build_base(self, filter_num, res):\n",
    "        input_tensor = layers.Input(shape=(res, res, filter_num), name=f\"d_{res}\")\n",
    "        x = minibatch_std(input_tensor)\n",
    "        x = EqualizedConv(filter_num, 3)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = EqualizedDense(filter_num)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = EqualizedDense(1)(x)\n",
    "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
    "\n",
    "    def build_block(self, filter_num_1, filter_num_2, res):\n",
    "        input_tensor = layers.Input(shape=(res, res, filter_num_1), name=f\"d_{res}\")\n",
    "        x = EqualizedConv(filter_num_1, 3)(input_tensor)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = EqualizedConv(filter_num_2)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.AveragePooling2D((2, 2))(x)\n",
    "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
    "\n",
    "    def grow(self, res_log2):\n",
    "        res = 2 ** res_log2\n",
    "        idx = res_log2 - self.start_res_log2\n",
    "        alpha = layers.Input(shape=(1), name=\"d_alpha\")\n",
    "        input_image = layers.Input(shape=(res, res, 3), name=\"input_image\")\n",
    "        x = self.from_rgb[idx](input_image)\n",
    "        x = self.d_blocks[idx](x)\n",
    "        if idx > 0:\n",
    "            idx -= 1\n",
    "            downsized_image = layers.AveragePooling2D((2, 2))(input_image)\n",
    "            y = self.from_rgb[idx](downsized_image)\n",
    "            x = fade_in(alpha[0], x, y)\n",
    "\n",
    "            for i in range(idx, -1, -1):\n",
    "                x = self.d_blocks[i](x)\n",
    "        return keras.Model([input_image, alpha], x, name=f\"discriminator_{res}_x_{res}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:14.659849800Z",
     "start_time": "2023-05-12T19:31:14.612851100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class StyleGAN(tf.keras.Model):\n",
    "    def __init__(self, z_dim=512, target_res=64, start_res=4):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.target_res_log2 = log2(target_res)\n",
    "        self.start_res_log2 = log2(start_res)\n",
    "        self.current_res_log2 = self.target_res_log2\n",
    "        self.num_stages = self.target_res_log2 - self.start_res_log2 + 1\n",
    "\n",
    "        self.alpha = tf.Variable(1.0, dtype=tf.float32, trainable=False, name=\"alpha\")\n",
    "\n",
    "        self.mapping = Mapping(num_stages=self.num_stages)\n",
    "        self.d_builder = Discriminator(self.start_res_log2, self.target_res_log2)\n",
    "        self.g_builder = Generator(self.start_res_log2, self.target_res_log2)\n",
    "        self.g_input_shape = self.g_builder.input_shape\n",
    "\n",
    "        self.phase = None\n",
    "        self.train_step_counter = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "\n",
    "        self.loss_weights = {\"gradient_penalty\": 10, \"drift\": 0.001}\n",
    "\n",
    "    def grow_model(self, res):\n",
    "        tf.keras.backend.clear_session()\n",
    "        res_log2 = log2(res)\n",
    "        self.generator = self.g_builder.grow(res_log2)\n",
    "        self.discriminator = self.d_builder.grow(res_log2)\n",
    "        self.current_res_log2 = res_log2\n",
    "        print(f\"\\nModel resolution:{res}x{res}\")\n",
    "\n",
    "    def compile(\n",
    "        self, steps_per_epoch, phase, res, d_optimizer, g_optimizer, *args, **kwargs\n",
    "    ):\n",
    "        self.loss_weights = kwargs.pop(\"loss_weights\", self.loss_weights)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        if res != 2 ** self.current_res_log2:\n",
    "            self.grow_model(res)\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "\n",
    "        self.train_step_counter.assign(0)\n",
    "        self.phase = phase\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "        super().compile(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def generate_noise(self, batch_size):\n",
    "        noise = [\n",
    "            tf.random.normal((batch_size, 2 ** res, 2 ** res, 1))\n",
    "            for res in range(self.start_res_log2, self.target_res_log2 + 1)\n",
    "        ]\n",
    "        return noise\n",
    "\n",
    "    def gradient_loss(self, grad):\n",
    "        loss = tf.square(grad)\n",
    "        loss = tf.reduce_sum(loss, axis=tf.range(1, tf.size(tf.shape(loss))))\n",
    "        loss = tf.sqrt(loss)\n",
    "        loss = tf.reduce_mean(tf.square(loss - 1))\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "\n",
    "        self.train_step_counter.assign_add(1)\n",
    "\n",
    "        if self.phase == \"TRANSITION\":\n",
    "            self.alpha.assign(\n",
    "                tf.cast(self.train_step_counter / self.steps_per_epoch, tf.float32)\n",
    "            )\n",
    "        elif self.phase == \"STABLE\":\n",
    "            self.alpha.assign(1.0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        alpha = tf.expand_dims(self.alpha, 0)\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        real_labels = tf.ones(batch_size)\n",
    "        fake_labels = -tf.ones(batch_size)\n",
    "\n",
    "        z = tf.random.normal((batch_size, self.z_dim))\n",
    "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
    "        noise = self.generate_noise(batch_size)\n",
    "\n",
    "        # generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            w = self.mapping(z)\n",
    "            fake_images = self.generator([const_input, w, noise, alpha])\n",
    "            pred_fake = self.discriminator([fake_images, alpha])\n",
    "            g_loss = wasserstein_loss(real_labels, pred_fake)\n",
    "\n",
    "            trainable_weights = (\n",
    "                self.mapping.trainable_weights + self.generator.trainable_weights\n",
    "            )\n",
    "            gradients = g_tape.gradient(g_loss, trainable_weights)\n",
    "            self.g_optimizer.apply_gradients(zip(gradients, trainable_weights))\n",
    "\n",
    "        # discriminator\n",
    "        with tf.GradientTape() as gradient_tape, tf.GradientTape() as total_tape:\n",
    "            # forward pass\n",
    "            pred_fake = self.discriminator([fake_images, alpha])\n",
    "            pred_real = self.discriminator([real_images, alpha])\n",
    "\n",
    "            epsilon = tf.random.uniform((batch_size, 1, 1, 1))\n",
    "            interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "            gradient_tape.watch(interpolates)\n",
    "            pred_fake_grad = self.discriminator([interpolates, alpha])\n",
    "\n",
    "            # calculate losses\n",
    "            loss_fake = wasserstein_loss(fake_labels, pred_fake)\n",
    "            loss_real = wasserstein_loss(real_labels, pred_real)\n",
    "            loss_fake_grad = wasserstein_loss(fake_labels, pred_fake_grad)\n",
    "\n",
    "            # gradient penalty\n",
    "            gradients_fake = gradient_tape.gradient(loss_fake_grad, [interpolates])\n",
    "            gradient_penalty = self.loss_weights[\n",
    "                \"gradient_penalty\"\n",
    "            ] * self.gradient_loss(gradients_fake)\n",
    "\n",
    "            # drift loss\n",
    "            all_pred = tf.concat([pred_fake, pred_real], axis=0)\n",
    "            drift_loss = self.loss_weights[\"drift\"] * tf.reduce_mean(all_pred ** 2)\n",
    "\n",
    "            d_loss = loss_fake + loss_real + gradient_penalty + drift_loss\n",
    "\n",
    "            gradients = total_tape.gradient(\n",
    "                d_loss, self.discriminator.trainable_weights\n",
    "            )\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(gradients, self.discriminator.trainable_weights)\n",
    "            )\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs: dict()):\n",
    "        style_code = inputs.get(\"style_code\", None)\n",
    "        z = inputs.get(\"z\", None)\n",
    "        noise = inputs.get(\"noise\", None)\n",
    "        batch_size = inputs.get(\"batch_size\", 1)\n",
    "        alpha = inputs.get(\"alpha\", 1.0)\n",
    "        alpha = tf.expand_dims(alpha, 0)\n",
    "        if style_code is None:\n",
    "            if z is None:\n",
    "                z = tf.random.normal((batch_size, self.z_dim))\n",
    "            style_code = self.mapping(z)\n",
    "\n",
    "        if noise is None:\n",
    "            noise = self.generate_noise(batch_size)\n",
    "\n",
    "        # self.alpha.assign(alpha)\n",
    "\n",
    "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
    "        images = self.generator([const_input, style_code, noise, alpha])\n",
    "        images = np.clip((images * 0.5 + 0.5) * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        return images\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:14.719850600Z",
     "start_time": "2023-05-12T19:31:14.672850600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "START_RES = 4\n",
    "TARGET_RES = 128\n",
    "\n",
    "style_gan = StyleGAN(start_res=START_RES, target_res=TARGET_RES)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:17.804334500Z",
     "start_time": "2023-05-12T19:31:14.721350300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train(\n",
    "    start_res=START_RES,\n",
    "    target_res=TARGET_RES,\n",
    "    steps_per_epoch=5000,\n",
    "    display_images=True,\n",
    "):\n",
    "    opt_cfg = {\"learning_rate\": 1e-3, \"beta_1\": 0.0, \"beta_2\": 0.99, \"epsilon\": 1e-8}\n",
    "\n",
    "    val_batch_size = 16\n",
    "    val_z = tf.random.normal((val_batch_size, style_gan.z_dim))\n",
    "    val_noise = style_gan.generate_noise(val_batch_size)\n",
    "\n",
    "    start_res_log2 = int(np.log2(start_res))\n",
    "    target_res_log2 = int(np.log2(target_res))\n",
    "\n",
    "    for res_log2 in range(start_res_log2, target_res_log2 + 1):\n",
    "        res = 2 ** res_log2\n",
    "        for phase in [\"TRANSITION\", \"STABLE\"]:\n",
    "            if res == start_res and phase == \"TRANSITION\":\n",
    "                continue\n",
    "\n",
    "            train_dl = create_dataloader(res)\n",
    "\n",
    "            steps = int(train_step_ratio[res_log2] * steps_per_epoch)\n",
    "\n",
    "            style_gan.compile(\n",
    "                d_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
    "                g_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
    "                loss_weights={\"gradient_penalty\": 10, \"drift\": 0.001},\n",
    "                steps_per_epoch=steps,\n",
    "                res=res,\n",
    "                phase=phase,\n",
    "                run_eagerly=False,\n",
    "            )\n",
    "\n",
    "            prefix = f\"res_{res}x{res}_{style_gan.phase}\"\n",
    "\n",
    "            ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "                f\"checkpoints/stylegan_{res}x{res}.ckpt\",\n",
    "                save_weights_only=True,\n",
    "                verbose=0,\n",
    "            )\n",
    "            print(phase)\n",
    "            style_gan.fit(\n",
    "                train_dl, epochs=10, steps_per_epoch=steps, callbacks=[ckpt_cb]\n",
    "            )\n",
    "\n",
    "            if display_images:\n",
    "                images = style_gan({\"z\": val_z, \"noise\": val_noise, \"alpha\": 1.0})\n",
    "                plot_images(images, res_log2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T19:31:17.819333500Z",
     "start_time": "2023-05-12T19:31:17.811832800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model resolution:4x4\n",
      "STABLE\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 28s 388ms/step - d_loss: -11.5285 - g_loss: 13.1956\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 18s 368ms/step - d_loss: -18.8498 - g_loss: 25.7006\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 19s 379ms/step - d_loss: -15.4876 - g_loss: 19.8864\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 20s 406ms/step - d_loss: -13.7689 - g_loss: 16.4153\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 19s 391ms/step - d_loss: -12.6907 - g_loss: 14.6019\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 19s 388ms/step - d_loss: -11.2729 - g_loss: 11.5685\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 20s 392ms/step - d_loss: -10.4659 - g_loss: 9.9942\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 19s 384ms/step - d_loss: -9.7114 - g_loss: 8.7150\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 19s 371ms/step - d_loss: -9.0727 - g_loss: 7.6350\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 19s 385ms/step - d_loss: -8.2758 - g_loss: 6.0224\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x50 with 16 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model resolution:8x8\n",
      "TRANSITION\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 92s 2s/step - d_loss: -26.9786 - g_loss: 31.8301\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 79s 2s/step - d_loss: -84.9771 - g_loss: 127.2402\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 73s 1s/step - d_loss: -144.4530 - g_loss: 219.9134\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 72s 1s/step - d_loss: -199.3820 - g_loss: 289.2416\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 67s 1s/step - d_loss: -249.4110 - g_loss: 324.0295\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 67s 1s/step - d_loss: -299.8996 - g_loss: 348.5994\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 70s 1s/step - d_loss: -320.6091 - g_loss: 358.3722\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 66s 1s/step - d_loss: -375.3089 - g_loss: 392.8474\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 67s 1s/step - d_loss: -380.9714 - g_loss: 395.1752\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 68s 1s/step - d_loss: -437.0500 - g_loss: 439.9569\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1600x100 with 16 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STABLE\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 70s 1s/step - d_loss: -6.9557 - g_loss: -10.5621\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 64s 1s/step - d_loss: -5.9798 - g_loss: -12.8691\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 66s 1s/step - d_loss: -12.7044 - g_loss: -38.4840\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 65s 1s/step - d_loss: -14.0373 - g_loss: -25.6009\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 68s 1s/step - d_loss: -14.8631 - g_loss: -13.5004\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 65s 1s/step - d_loss: -15.1798 - g_loss: -4.5244\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 65s 1s/step - d_loss: -15.2265 - g_loss: 2.1457\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 68s 1s/step - d_loss: -15.5423 - g_loss: 7.8866\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 68s 1s/step - d_loss: -14.8052 - g_loss: 8.8603\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 68s 1s/step - d_loss: -14.4775 - g_loss: 9.0361\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1600x100 with 16 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model resolution:16x16\n",
      "TRANSITION\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 202s 4s/step - d_loss: -52.1391 - g_loss: 62.9558\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 194s 4s/step - d_loss: -162.1219 - g_loss: 215.2265\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 187s 4s/step - d_loss: -247.9900 - g_loss: 370.0863\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 184s 4s/step - d_loss: -245.2619 - g_loss: 473.7762\n",
      "Epoch 5/10\n",
      "41/50 [=======================>......] - ETA: 32s - d_loss: -391.1985 - g_loss: 530.3760"
     ]
    }
   ],
   "source": [
    "train(start_res=4, target_res=256, steps_per_epoch=50, display_images=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-12T19:31:17.823332700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = \"https://github.com/soon-yau/stylegan_keras/releases/download/keras_example_v1.0/stylegan_128x128.ckpt.zip\"\n",
    "\n",
    "weights_path = keras.utils.get_file(\n",
    "    \"stylegan_128x128.ckpt.zip\",\n",
    "    url,\n",
    "    extract=True,\n",
    "    cache_dir=os.path.abspath(\".\"),\n",
    "    cache_subdir=\"pretrained\",\n",
    ")\n",
    "\n",
    "style_gan.grow_model(256)\n",
    "style_gan.load_weights(os.path.join(\"pretrained/stylegan_128x128.ckpt\"))\n",
    "\n",
    "tf.random.set_seed(196)\n",
    "batch_size = 2\n",
    "z = tf.random.normal((batch_size, style_gan.z_dim))\n",
    "w = style_gan.mapping(z)\n",
    "noise = style_gan.generate_noise(batch_size=batch_size)\n",
    "images = style_gan({\"style_code\": w, \"noise\": noise, \"alpha\": 1.0})\n",
    "plot_images(images, 5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
