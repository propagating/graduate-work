---
title: "CS624 HW1"
author: "Ryan Richardson"
date: "29/09/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

## Problem 1


### Part A

#### Normality

```{r checkNormality}
firstTest = data.frame("Method" = c("1","1","1","1","1","1"), "Score" = c(79,66,57,91,42,59))
secondTest = data.frame("Method" = c("2", "2", "2", "2", "2", "2"), "Score" = c(71,43,58,78,20,56))
firstTest$Method = as.factor(firstTest$Method)
secondTest$Method = as.factor(secondTest$Method)

shapiro.test(firstTest$Score)
shapiro.test(secondTest$Score)

hist(firstTest$Score)
hist(secondTest$Score)
```

Shapiro-Wilk Test for Normality has a p-value > .05 for both sets of scores, we can assume they are normally distributed. Plotting a histogram of both sets of scores also suggests normality, though there may ben an outlier on in the first set of points.

#### Null Hypothesis
The null hypothess is that the mean scores of the first 6 students are not different from the means of the second 6 students.

$$H_0: \mu_1 = \mu_2$$ 
$$H_a: \mu_1 \neq \mu_2$$ 

#### Two Sample T-Test & Findings

Findings: Fail to reject $H_0: \mu_1 = \mu_2$ with a p-value of $.3298$ at $\alpha = 0.01$

Confidence Interval (Difference of Means) : $(-23.93371, 46.60038)$

By failing to reject, we assume that there is no difference in the mean scores between the two teaching methods on different populations of students. Even so, the confidence interval suggests that the true value of the difference between teaching methods is somewhere between -23.93 and 46.60.

```{r twoSampleTest}

t.test(firstTest$Score, secondTest$Score , paired = FALSE, conf.level = 0.99, mu = 0)
```


### Part B

#### Normality

We start by taking the differences between the scores from Method 1 and the scores form Method 2 for the same students. Running the differences through Shapiro-Wilkes test we get a p-value of .55, which allows us to assume normality for the paired t-test. However, the histogram of the differences looks like it is uniformly distrbuted, except for a single outlier.  
```{r pairedTestNormality}
differences = firstTest$Score - secondTest$Score
shapiro.test(differences)
hist(differences)
```
#### Null Hypothesis
The null hypothess is that the mean scores of the first test that the 6 students take is different from the mean scores of the second test that the same 6 students take at a later date.

$$H_0: \mu_{s_1} = \mu_{s_2}$$ 
$$H_a: \mu_{s_1} \neq \mu_{s_2}$$ 

#### Two Sample T-Test & Findings

Findings: Fail to reject $H_0: \mu_1 = \mu_2$ with a p-value of $0.03721$ at $\alpha = .01$

Confidence Interval (Difference of Means) : $(-4.884554, 27.551231)$

By failing to reject, we assume that there is no difference in the mean scores between the two teaching methods amongst the same students. Even so, the confidence interval suggests that the true value of the difference between teaching methods is somewhere between -4.89 and 27.55. If we were to use $\alpha = 0.95$, our findings would reject the null and would show a difference between teaching methods. 

```{r pairedTest}
t.test(firstTest$Score, secondTest$Score , paired = TRUE, conf.level = 0.99, mu = 0)
```


### Part C


With a p-value of $0.098$ we reject the null hypothesis of $\mu_1 = 80$,  at $\alpha = .10$ significance.

Confidence Interval for the true mean is $(47.49, 83.84)$ at 90% confidence/

```{r singleSide}
t.test(firstTest$Score, mu = 80)
```

### Part D

Both result say that there is no significant difference in the teaching methods when $\alpha = 0.01$. However, the results in part b do show a signficant difference at $\alpha = 0.05$ suggesting that the averages of the two samples are different. If we assume the calculted means are correct, then Method 1 is the better teaching method due to the higher mean. 


There is very little data about how the teaching mehtods differ, the test content, how students perform in a related subject area, study time, student demographics, etc. In order to provide a better model, I think it would be useful to pull the student's class schedules, demographic information, and at least their grades from other classes in order to get a more accurate idea of what is actually influencing the student's grade. 

I would also like to see multiple groups of students, one that was tested multiple times using Method 1 only and one that was retesetd multiple times using Method 2 only, and then to compare them. We can use that to get an estimate of how much students improve, or regess, at a base level for each method over time.



## Qusetion 2

### Part A

All of the p-values are below the taget level, $\alpha = 0.05$. as a result, we reject the null hypothesis that weight is normally distributed for any of the target groups.

```{r wcgsSetup, echo=TRUE, message=TRUE, warning=TRUE}

library(foreign)
wcgs <- read.dta('data/wcgs.dta')
wcgs$typchd69 = as.factor(wcgs$typchd69)
for(i in levels(wcgs$typchd69)){
   obs = wcgs[which(wcgs$typchd69 == i),]
   print(i)
   print(shapiro.test(obs$weight))
   hist(obs$weight)
}

```


### Part B

The Bartlett test returns a p-value of .03791, greater than the target level $\alpha=0.05$, meaning we fail to reject the null and can assume the variances between the groups are homogenous. 
```{r homogeneity}
bartlett.test(wcgs$weight ~ wcgs$typchd69)
```


### Part C


#### ANOVA
With ANOVA we get a p-value of 0.0018 less than the target level $\alpha=0.05$, which means we reject the null that there is a difference between the means of the 4 groups. This is supported by a boxplot across the 4 groups.
```{r anovaMeans}
library(ggplot2)
weightAnova = aov(weight ~ typchd69, data=wcgs)
summary.aov(weightAnova)
ggplot(wcgs, aes(x=typchd69, y=weight)) + geom_boxplot()

```


#### Bonferroni


The Bonferroni Adjusted Pairwise T-Test shows that there is only a significant difference at $\alpha = 0.05$ between groups 0 and 2 with a p value of .043 
```{r adjustedPairwise}
pairwise.t.test(wcgs$weight, wcgs$typchd69, p.adjust = "bonferroni")
```


#### Tukey's Honest Differences


Tukey's HSD also contains only one p-value below our target level $\alpha = 0.05$, suggesting there is only a single signficant differences between the means of group 0 and group 2 across the four groups.

```{r tukey}
TukeyHSD(weightAnova)
```


### Part D

NOT DONE BECAUSE ANOVA DID NOT FAIL TO REJECT THE NULL


## Question 3

### Part A

$$H_0 : \mu_1 = \mu_2 = \mu3 = \mu4$$
$$H_a : \mu_1 \neq \mu_2 \neq \mu3  \neq \mu4$$

```{r q3Setup}
diets <- read.table('data/diets.txt', header=TRUE)
diets$Diet = as.factor(diets$Diet)
```


### Part B

Shapiro-Wilks returns a p-value of 0.3985, greater than our target $\alpha=0.05$. We fail to reject the null and can assume that our weightloss data is not singificantly different from a normal distribution. 

By extending the Shapiro test to look at normality across individual diet, we get even higher p-values across each group suggesting that each group is normally distributed as well. 

Bartlett test for homogenous variance bewteen the different diets returns a p-value of 0.5482, greater than our target $\alpha=0.05$. Which means that we fail to reject the null and cannot assume the variances are homogenous. 

Despite this, we should still be able to conduct an ANOVA as the samples should be independently distributed across each separate diet. 
```{r anovaAssumtpionTests}
library(RVAideMemoire)
shapiro <- shapiro.test(diets$WeightLoss)
shapiro

shapByFactor <- byf.shapiro(WeightLoss~Diet, data = diets)
shapByFactor

bart <- bartlett.test(WeightLoss ~ Diet, data = diets)
bart
```




### Part C

Running ANOVA using tapply and mean() we get a p-value of  1.255527e-12, much less than our target $\alpha=0.05$ which means that we reject the null that the mean between diets is the same. Essentially saying that there is a difference between the mean weight across the diets.

```{r anovaByHand}
totalObservations = length(diets$WeightLoss)
totalLevels = length(levels(diets$Diet))

groupMeans <- tapply(diets$WeightLoss, diets$Diet, mean)
groupMeans

groupCounts <- tapply(diets$WeightLoss, diets$Diet, length)
groupCounts

overallMeans <- mean(groupMeans)
overallMeans

sst =  sum(groupCounts * ((groupMeans - overallMeans)^2))
sst

mst = sst/(totalLevels - 1)
mst

groupVariances =  tapply(diets$WeightLoss, diets$Diet, var)
groupVariances

sse = sum(groupVariances * (groupCounts - 1))
sse
mse = sse / (totalObservations - totalLevels)
mse

fStat = mst/mse
fStat

pf(fStat, 3, 114, lower.tail = FALSE)

```

### Part D

Running a Bonferroni Adjusted Pairwise comparison shows that there is a significant differences between every diet level except between Diets 1 and 2 at $\alpha = 0.05$ 
```{r dietBonf}
pairwise.t.test(diets$WeightLoss, diets$Diet, p.adjust = "bonferroni")
```
### Part E & F

Runing the standard Anova, the Sum Squared Error value was a bit less than the Sum Squared Error value I calculated. This may be due to some additional measures taken when SSE is calculated, such as the removal of outliers. The result was a difference of 0.6 between the my FSTAT  (25.32) and the FSTAT of the AOV Package (24.75). All in all, they were extremely close.   


Running TukeyHSD we see significant differences between every diet level except between Diets 1 and 2 at $\alpha = 0.05$ 
```{r anovaComparison}
dietAnova = aov(WeightLoss ~ Diet, data = diets)
summary(dietAnova )
TukeyHSD(dietAnova)
```

### Part G

Without the Bonferooni adjustment, we still see a signifiant difference between teh means of each diet except for diets 1 and 2 at $\alpha = 0.05$, but are drastically different compaared to teh adjustment, especially between groups 1 and 4, groups 2 and 4, groups 3 and 1, groups 2 and 1, and groups 2 and 3. 
```{r dietPairwise}
pairwise.t.test(diets$WeightLoss, diets$Diet)
```


## Part 4

We have 16 total runs with 8 runs of A and 8 runs of B. When calculating the Wald Wolfowitz P Value we get p = 0.0001554 which is significantly less than our target $\alpha = 0.05$ as a result, we fail to reject the null that P(T = 16) and can assume the sequence is random. 
```{r waldWalfTest}
library(stringr)
library(adehabitatLT)
stringSeq = 'aabbaaaabbbaababbaaabbbbbababbbabbb'
inputSeq = as.list(strsplit(stringSeq, "")[[1]])
convertedRuns = str_count(inputSeq, 'a') # convert to 1s and 0s to work with RLE
runLengths = rle(convertedRuns)
totalRuns = length(rle(convertedRuns)$lengths)
aChoose = choose(7,(totalRuns/2) - 1)
bChoose = choose(7,(totalRuns/2) - 1)
abChoose = choose((8+8), 8)
pv = (2 * aChoose * bChoose)/abChoose
pv

wawotest(convertedRuns)
```
