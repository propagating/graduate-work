{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Quiz2/HW3 Hackathon\n",
    "\n",
    "Team name: HONEYBADGERRS\n",
    "\n",
    "Member names: Ryan Richardson, Jarret Guillow, Grace Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this Jupyter Notebook as a template to get started. Before you start, here are a few hints that might help you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate your solution based on a simple first submission\n",
    "\n",
    "For the quiz, try to use the numerical variables first. Fit a regression model on the numerical variables first and create a submission quickly. You can then try various other things and iterate your code based on the simple solution. Use Google to find code to solve the problems you encounterâ€”e.g., how to select only the categorical variables in the DataFrame, how to impute missing data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "### NAs (missing values) in both training set and test set\n",
    "\n",
    "In the dataset you are working with in this competition, you will find that both the training set and test set have NAs (missing values). If the test set has missing values, your model will not be able to predict on the entire set. How do you deal with it? (Hint: one idea is to concatenate the training set and test set, fill in NAs with the means/median/mode or whatever you choose, then split them up again)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "\n",
    "We have not discussed how to deal with categorical variables. They do not fit that easily into regression models. But how do we fully utilize them nontheless? (Hint: convert them into dummy variables, typically using one-hot encoding. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Your model will be evaluated using its Root Mean Squared Error (MSE) on the test set. (As usual, you do not have the labels on the test set, of course; so you cannot directly measure the RMSE on it.) You can calculate the MSE on a dataset using scikit-learn https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html. Then take the squared root of the MSE to get the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Id and rename columns\n",
    "def trim_test_cols(df):\n",
    "    df.columns.str.contains('^id', case=False)\n",
    "    trimmed = df.loc[:, ~df.columns.str.contains('^id', case=False)]\n",
    "\n",
    "    trimmed.columns = ['SubClass', 'Zoning', 'Frontage', 'Area', 'StreetType', 'AlleyType', 'Shape', 'Contour', 'Utilities',\n",
    "                       'Configuration', 'Slope', 'Neighborhood', 'FirstCondition', 'SecondCondition', 'Type', 'Style', 'TotalQuality',\n",
    "                       'TotalCondition', 'YearBuilt', 'YearRemodeled', 'Roof', 'RoofMaterial', 'FirstExterior', 'SecondExterior',\n",
    "                       'VeneerType', 'VeneerArea', 'ExteriorQuality', 'ExteriorCondition', 'Foundation', 'BasementQuality',\n",
    "                       'BasementCondition', 'BasementExposure', 'FirstBasementFinish', 'FirstBasementSize', 'SecondBasementFinish',\n",
    "                       'SecondBasementSize', 'BasementUnfinishedSize', 'TotalBasementSize', 'Heating', 'HeatingQuality', 'CentralAir',\n",
    "                       'Electrical', 'FirstFloorSize', 'SecondFloorSize', 'LowQualityFinishedSize', 'AboveGradeArea',\n",
    "                       'BasementFullBath', 'BasementHalfBath', 'HouseFullBath', 'HouseHalfBath', 'BedroomsAboveGrade',\n",
    "                       'KitchenAboveGrade', 'KitchenQuality', 'TotalRoomsAboveGrade', 'Functional', 'Fireplace', 'FireplaceQuality',\n",
    "                       'GarageType', 'GarageYear', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQuality', 'GarageCondition',\n",
    "                       'PavedDriveway', 'WoodDeckSize', 'OpenPorchSize', 'EnclosedPorch', 'AllSeasonPorch', 'ScreenPorch', 'PoolArea',\n",
    "                       'PoolQuality', 'Fence', 'MiscFeature', 'MiscValue', 'MonthSold', 'YearSold', 'SaleType', 'SaleCondition']\n",
    "    return trimmed\n",
    "\n",
    "\n",
    "# drop Id and rename columns\n",
    "def trim_train_cols(df):\n",
    "    df.columns.str.contains('^id', case=False)\n",
    "    trimmed = df.loc[:, ~df.columns.str.contains('^id', case=False)]\n",
    "\n",
    "    trimmed.columns = ['SubClass', 'Zoning', 'Frontage', 'Area', 'StreetType', 'AlleyType', 'Shape', 'Contour', 'Utilities',\n",
    "                       'Configuration', 'Slope', 'Neighborhood', 'FirstCondition', 'SecondCondition', 'Type', 'Style', 'TotalQuality',\n",
    "                       'TotalCondition', 'YearBuilt', 'YearRemodeled', 'Roof', 'RoofMaterial', 'FirstExterior', 'SecondExterior',\n",
    "                       'VeneerType', 'VeneerArea', 'ExteriorQuality', 'ExteriorCondition', 'Foundation', 'BasementQuality',\n",
    "                       'BasementCondition', 'BasementExposure', 'FirstBasementFinish', 'FirstBasementSize', 'SecondBasementFinish',\n",
    "                       'SecondBasementSize', 'BasementUnfinishedSize', 'TotalBasementSize', 'Heating', 'HeatingQuality', 'CentralAir',\n",
    "                       'Electrical', 'FirstFloorSize', 'SecondFloorSize', 'LowQualityFinishedSize', 'AboveGradeArea',\n",
    "                       'BasementFullBath', 'BasementHalfBath', 'HouseFullBath', 'HouseHalfBath', 'BedroomsAboveGrade',\n",
    "                       'KitchenAboveGrade', 'KitchenQuality', 'TotalRoomsAboveGrade', 'Functional', 'Fireplace', 'FireplaceQuality',\n",
    "                       'GarageType', 'GarageYear', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQuality', 'GarageCondition',\n",
    "                       'PavedDriveway', 'WoodDeckSize', 'OpenPorchSize', 'EnclosedPorch', 'AllSeasonPorch', 'ScreenPorch', 'PoolArea',\n",
    "                       'PoolQuality', 'Fence', 'MiscFeature', 'MiscValue', 'MonthSold', 'YearSold', 'SaleType', 'SaleCondition',\n",
    "                       'SalePrice']\n",
    "    return trimmed\n",
    "\n",
    "\n",
    "# convert categorical variables and populate their categorical breakdowns\n",
    "def convert_categories(df):\n",
    "    rank5Levels = ['Ex', 'Gd', 'TA', 'Fa', 'Po']\n",
    "    rank5Exposure = ['Gd', 'Av', 'Mn', 'No', 'NA']\n",
    "    rank6Levels = ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA']\n",
    "    rank7Finish = ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'NA']\n",
    "    functionalLevels = ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal']\n",
    "    garageFinish = ['Fin', 'RFn', 'Unf', 'NA']\n",
    "    poolLevels = ['Ex', 'Gd', 'TA', 'Fa', 'NA']\n",
    "    fenceLevels = ['GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'NA']\n",
    "    saleLevels = ['Normal', 'Abnormal', 'AdjLand', 'Alloca', 'Family Sale', 'Partial']\n",
    "\n",
    "    df.SubClass = pd.Categorical(df.SubClass, categories=[20, 30, 40, 45, 50, 60, 70, 75, 80, 85, 90, 120, 150, 160, 180, 190])\n",
    "    df.Zoning = pd.Categorical(df.Zoning, categories=['A (agr)', 'C (all)', 'FV', 'I (all)', 'RH', 'RL', 'RM', 'RP'])\n",
    "    df.StreetType = pd.Categorical(df.StreetType, categories=['Grvl', 'Pave'])\n",
    "    df.AlleyType = pd.Categorical(df.AlleyType, categories=['Grvl', 'Pave', 'NA'])\n",
    "    df.Shape = pd.Categorical(df.Shape, categories=['IR1', 'Reg', 'IR2', 'IR3'])\n",
    "    df.Contour = pd.Categorical(df.Contour, categories=['Lvl', 'Bnk', 'Low', 'HLS'])\n",
    "    df.Utilities = pd.Categorical(df.Utilities, categories=['AllPub', 'NoSewr', 'NoSeWa', 'ELO'])\n",
    "    df.Configuration = pd.Categorical(df.Configuration, categories=['Inside', 'FR2', 'CulDSac', 'Corner', 'FR3'])\n",
    "    df.Slope = pd.Categorical(df.Slope, categories=['Gtl', 'Mod', 'Sev'])\n",
    "    df.Neighborhood = pd.Categorical(df.Neighborhood,\n",
    "                                     categories=['IDOTRR', 'Somerst', 'OldTown', 'CollgCr', 'NAmes', 'Mitchel', 'MeadowV', 'Sawyer',\n",
    "                                                 'BrDale', 'Edwards', 'NWAmes', 'BrkSide', 'ClearCr', 'Gilbert', 'Timber', 'SawyerW',\n",
    "                                                 'Crawfor', 'SWISU', 'NridgHt', 'Veenker', 'Blueste', 'NoRidge', 'StoneBr', 'Greens',\n",
    "                                                 'Blmngtn', 'NPkVill', 'GrnHill'])\n",
    "    df.FirstCondition = pd.Categorical(df.FirstCondition,\n",
    "                                       categories=['Norm', 'Feedr', 'Artery', 'RRAn', 'RRAe', 'PosA', 'PosN', 'RRNe', 'RRNn'])\n",
    "    df.SecondCondition = pd.Categorical(df.SecondCondition, categories=['Norm', 'Feedr', 'RRNn', 'PosN', 'Artery', 'PosA'])\n",
    "    df.Type = pd.Categorical(df.Type, categories=['1Fam', '2fmCon', 'Duplex', 'TwnhsE', 'Twnhs'])\n",
    "    df.Style = pd.Categorical(df.Style, categories=['1Story', '2Story', '1.5Fin', '1.5Unf', '2.5Unf', 'SLvl', 'SFoyer', '2.5Fin'])\n",
    "    df.Roof = pd.Categorical(df.Roof, categories=['Gable', 'Hip', 'Gambrel', 'Flat', 'Mansard', 'Shed'])\n",
    "    df.RoofMaterial = pd.Categorical(df.RoofMaterial, categories=['CompShg', 'Tar&Grv', 'WdShake', 'WdShngl', 'Metal'])\n",
    "    df.FirstExterior = pd.Categorical(df.FirstExterior,\n",
    "                                      categories=['AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc',\n",
    "                                                  'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd', 'WdShing'])\n",
    "    df.SecondExterior = pd.Categorical(df.SecondExterior,\n",
    "                                       categories=['AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc',\n",
    "                                                   'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd', 'WdShing'])\n",
    "    df.VeneerType = pd.Categorical(df.VeneerType, categories=['BrkCmn', 'BrkFace', 'CBlock', 'None', 'Stone'])\n",
    "    df.Foundation = pd.Categorical(df.Foundation, categories=['BrkTil', 'CBlock', 'PConc', 'Slab', 'Stone', 'Wood'])\n",
    "    df.Heating = pd.Categorical(df.Heating, categories=['Floor', 'GasA', 'GasW', 'Grav', 'OthW', 'Wall', ])\n",
    "    df.CentralAir = pd.Categorical(df.CentralAir, categories=['Y', 'N'])\n",
    "    df.Electrical = pd.Categorical(df.Electrical, categories=['SBrkr', 'FuseA', 'FuseF', 'FuseP', 'Mix'])\n",
    "    df.GarageType = pd.Categorical(df.GarageType, categories=['2Types', 'Attchd', 'Basment', 'BuiltIn', 'CarPort', 'Detchd', 'NA'])\n",
    "    df.SaleType = pd.Categorical(df.SaleType, categories=['WD', 'CWD', 'VWD', 'New', 'COD', 'Con', 'ConLw', 'ConLI', 'ConLD', 'Oth'])\n",
    "    df.PavedDriveway = pd.Categorical(df.PavedDriveway, categories=['Y', 'P', 'N'])\n",
    "    df.MiscFeature = pd.Categorical(df.MiscFeature, categories=['Elev', 'Gar2', 'Othr', 'Shed', 'TenC', 'NA'])\n",
    "\n",
    "    df.TotalQuality = pd.Categorical(df.TotalQuality, ordered=True)\n",
    "    df.TotalCondition = pd.Categorical(df.TotalCondition, ordered=True)\n",
    "    df.ExteriorQuality = pd.Categorical(df.ExteriorQuality, ordered=True, categories=rank5Levels)\n",
    "    df.ExteriorCondition = pd.Categorical(df.ExteriorCondition, ordered=True, categories=rank5Levels)\n",
    "    df.BasementQuality = pd.Categorical(df.BasementQuality, ordered=True, categories=rank6Levels)\n",
    "    df.BasementCondition = pd.Categorical(df.BasementCondition, ordered=True, categories=rank6Levels)\n",
    "    df.BasementExposure = pd.Categorical(df.BasementExposure, ordered=True, categories=rank5Exposure)\n",
    "    df.FirstBasementFinish = pd.Categorical(df.FirstBasementFinish, ordered=True, categories=rank7Finish)\n",
    "    df.SecondBasementFinish = pd.Categorical(df.SecondBasementFinish, ordered=True, categories=rank7Finish)\n",
    "    df.HeatingQuality = pd.Categorical(df.HeatingQuality, ordered=True, categories=rank5Levels)\n",
    "    df.KitchenQuality = pd.Categorical(df.KitchenQuality, ordered=True, categories=rank5Levels)\n",
    "    df.Functional = pd.Categorical(df.Functional, ordered=True, categories=functionalLevels)\n",
    "    df.FireplaceQuality = pd.Categorical(df.FireplaceQuality, ordered=True, categories=rank6Levels)\n",
    "    df.GarageFinish = pd.Categorical(df.GarageFinish, ordered=True, categories=garageFinish)\n",
    "    df.GarageQuality = pd.Categorical(df.GarageQuality, ordered=True, categories=rank6Levels)\n",
    "    df.GarageCondition = pd.Categorical(df.GarageCondition, ordered=True, categories=rank6Levels)\n",
    "    df.PoolQuality = pd.Categorical(df.PoolQuality, ordered=True, categories=poolLevels)\n",
    "    df.Fence = pd.Categorical(df.Fence, ordered=True, categories=fenceLevels)\n",
    "    df.SaleCondition = pd.Categorical(df.SaleCondition, ordered=True, categories=saleLevels)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mse(original, fitted):\n",
    "    sum = 0\n",
    "    for i in range(0, len(original)):\n",
    "        diff = original.iloc[i] - fitted.iloc[i]\n",
    "        sum = sum + diff ** 2\n",
    "    MSE = sum / len(original)\n",
    "    return MSE\n",
    "\n",
    "\n",
    "# get indexes of numerical outliers\n",
    "def get_numerical_outliers(df, z_thresh=3):\n",
    "    constraints = df.apply(lambda x: np.abs(stats.zscore(x)) < z_thresh).all(axis=1)\n",
    "    outliers = df[~constraints].index\n",
    "    return outliers\n",
    "\n",
    "\n",
    "# fill non-categorical NaNs with values using iterative imputing\n",
    "def fill_numerical(df):\n",
    "    imp = IterativeImputer(missing_values=np.nan, sample_posterior=False,\n",
    "                           max_iter=10,\n",
    "                           n_nearest_features=4, initial_strategy='median')\n",
    "    transformedData = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return transformedData\n",
    "\n",
    "\n",
    "# fill categorical NaNs with most prevalent category, or create a new category based on number of missing values\n",
    "def fill_category(df):\n",
    "    for col in df.select_dtypes('category').columns:\n",
    "        dfCol = df.loc[:, col]\n",
    "        counts = dfCol.value_counts()\n",
    "        mostCommon = counts.index[0]\n",
    "        if counts.sum() < dfCol.size / 2:\n",
    "            if 'NA' in dfCol.cat.categories:\n",
    "                mostCommon = 'NA'\n",
    "            else:\n",
    "                dfCol.cat.add_categories(['NA'], inplace=True)\n",
    "                mostCommon = 'NA'\n",
    "        dfCol.fillna(mostCommon, inplace=True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default VIF calculation, adjusted for cases of over-fit and highly correlated results, set to 100 for easier number comparison\n",
    "def calculate_vif(r_squared):\n",
    "    if r_squared == 1:\n",
    "        return 100\n",
    "    vifValue = 1 / (1 - r_squared)\n",
    "    return vifValue\n",
    "\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "\n",
    "# create the VIF datatable\n",
    "def generate_vif(df):\n",
    "    numerics = df.select_dtypes('number')\n",
    "    numerics.columns.str.contains('^SalePrice', case=False)\n",
    "    numerics = numerics.loc[:, ~numerics.columns.str.contains('^SalePrice', case=False)]\n",
    "\n",
    "    vifFrame = pd.DataFrame(columns=['Variable', 'VIF'])\n",
    "\n",
    "    for i in range(0, len(numerics.columns)):\n",
    "        x = numerics.loc[:, numerics.columns != numerics.columns[i]]\n",
    "        y = numerics.loc[:, numerics.columns == numerics.columns[i]]\n",
    "\n",
    "        model = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "        vifValue = calculate_vif(model.rsquared)\n",
    "\n",
    "        vifRecord = {'Variable': numerics.columns[i], 'VIF': vifValue}\n",
    "        vifFrame = vifFrame.append(vifRecord, ignore_index=True)\n",
    "\n",
    "    # set categorical Vif to 1 for easier processing\n",
    "    categorical = df.select_dtypes('category')\n",
    "    for col in categorical.columns:\n",
    "        vifRecord = {'Variable': col, 'VIF': 1}\n",
    "        vifFrame = vifFrame.append(vifRecord, ignore_index=True)\n",
    "\n",
    "    return vifFrame\n",
    "\n",
    "\n",
    "def k_fold_validation(target, descriptors, folds):\n",
    "    ols = LinearRegression(fit_intercept=True)\n",
    "    scores = cross_val_score(ols, descriptors, target, cv=folds)\n",
    "    mean = scores.mean()\n",
    "    print(f'Validation Mean Score Results {mean}')\n",
    "    res = cross_val_score(ols, descriptors, target, cv=folds)\n",
    "    print(f'Validation OLS Results{res}')\n",
    "    predictedY = cross_val_predict(ols, descriptors, target, cv=folds)\n",
    "    print(f'Validation Predicted Y Result {predictedY.mean()}')\n",
    "\n",
    "\n",
    "def generate_cramers_v(df):\n",
    "    categories = df.select_dtypes('category')\n",
    "    for col in categories.columns:  # col = x\n",
    "        for col2 in categories.columns:  # col2 = y\n",
    "            if col2 != col:\n",
    "                score = cramers_v(categories[col], categories[col2])\n",
    "                if score > 0.5:\n",
    "                    print(col, col2)\n",
    "                    print(score)\n",
    "    return\n",
    "\n",
    "\n",
    "# remove categories which are overwhelmingly empty, or overwhelmingly contain the same values (Paved Street type with 1993 records)\n",
    "def remove_common_categories(df):\n",
    "    commonCategories = ['StreetType', 'AlleyType', 'Contour', 'Utilities', 'Slope', 'FirstCondition', 'SecondCondition', 'Type',\n",
    "                        'RoofMaterial', 'BasementCondition', 'SecondBasementFinish', 'Heating', 'Electrical', 'Functional', 'GarageQuality',\n",
    "                        'GarageCondition', 'PavedDriveway', 'PoolQuality', 'Fence', 'MiscFeature', 'SaleType', 'Zoning', 'SubClass',\n",
    "                        'GarageFinish', 'SecondExterior', 'ExteriorQuality', 'BasementExposure', 'FirstBasementFinish', 'HeatingQuality',\n",
    "                        'FirstExterior', 'SaleCondition', 'FireplaceQuality', 'Configuration', 'KitchenAboveGrade', 'Shape', 'Foundation',\n",
    "                        'KitchenQuality', 'ExteriorCondition', 'BasementQuality']\n",
    "    df.drop(commonCategories, axis=1, inplace=True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pre-processing pipeline\n",
    "def process_raw(train, remove_outliers):\n",
    "    # convert categorical variables and populate with categories\n",
    "    convert_categories(train)\n",
    "\n",
    "    # split out numerical and categorical variables\n",
    "    # fill missing categorical variables with the most prevalent, or create a new variable (NA) if more than 50% are missing values\n",
    "    fill_category(train)\n",
    "\n",
    "    # split out numerical variables and use iterative imputation to fill out NaNs\n",
    "    numericalData = fill_numerical(train.select_dtypes('number'))\n",
    "\n",
    "    for col in numericalData.columns:\n",
    "        numericalCol = numericalData.loc[:, col]\n",
    "        train.loc[:, col] = numericalCol\n",
    "\n",
    "    # retrieve indices of outlier rows\n",
    "    if remove_outliers:\n",
    "        outlierIndices = get_numerical_outliers(numericalData, 3)\n",
    "        train.drop(outlierIndices, inplace=True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ADJUST VIF AND P-VALUES HERE TO GET DIFFERENT R-SQ and ADJ-R-SQ Values\n",
    "p_threshold = 0.05\n",
    "vif_threshold = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainingSet = pd.read_csv('train.csv')\n",
    "trainingData = trim_train_cols(trainingSet)\n",
    "process_raw(trainingData, True)\n",
    "remove_common_categories(trainingData)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove variables with a VIF higher than the threshold\n",
    "vifScores = generate_vif(trainingData)\n",
    "generate_cramers_v(trainingData)\n",
    "\n",
    "lowVifScores = vifScores[vifScores.VIF <= vif_threshold]\n",
    "\n",
    "potentialDescriptors = trainingData[lowVifScores.Variable]\n",
    "\n",
    "print(potentialDescriptors.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to do OLS with categorical variables, we need to split out each category into its own column\n",
    "dummies = pd.get_dummies(potentialDescriptors.select_dtypes('category'), drop_first=True)\n",
    "numbers = potentialDescriptors.select_dtypes('number')\n",
    "descriptors = numbers.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first round, Adj-Rsq of .916\n",
    "results = sm.OLS(trainingData['SalePrice'], sm.add_constant(descriptors, has_constant='add')).fit()\n",
    "print(f'Results with all variables below VIF of {vif_threshold}')\n",
    "print(results.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop variables with p value higher than the threshold\n",
    "valuesToDropExist = results.pvalues > p_threshold\n",
    "valuesToDropExist = valuesToDropExist[valuesToDropExist]\n",
    "\n",
    "# drop variables with p value less than 0.07, Adj=Rsq of .881\n",
    "while len(valuesToDropExist) > 0:\n",
    "    descriptorsToKeep = results.pvalues <= p_threshold\n",
    "    descriptorsToKeep[0] = False\n",
    "    descriptorsToKeep = descriptorsToKeep[descriptorsToKeep]\n",
    "    descriptors = descriptors[descriptorsToKeep.keys()]\n",
    "    results = sm.OLS(trainingData['SalePrice'], sm.add_constant(descriptors, has_constant='add')).fit()\n",
    "    valuesToDropExist = results.pvalues > p_threshold\n",
    "    valuesToDropExist = valuesToDropExist[valuesToDropExist]\n",
    "\n",
    "print(f'Results with all variables below VIF of {vif_threshold}, and P-value below {p_threshold}')\n",
    "print(results.summary())\n",
    "\n",
    "MSE = generate_mse(trainingData['SalePrice'], results.fittedvalues)\n",
    "print(f'Pre-Validation {MSE}')\n",
    "\n",
    "k_fold_validation(trainingData['SalePrice'], descriptors, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testingSet = pd.read_csv('test.csv')\n",
    "testingData = trim_test_cols(testingSet)\n",
    "# don't drop outliers on test set\n",
    "process_raw(testingData, False)\n",
    "\n",
    "# in order to do OLS with categorical variables, we need to split out each category into its own column, this code should do that\n",
    "newDummies = pd.get_dummies(testingData.select_dtypes('category'), drop_first=True)\n",
    "newNumbers = testingData.select_dtypes('number')\n",
    "testDescriptors = newNumbers.join(newDummies)\n",
    "\n",
    "testDescriptors = testDescriptors.loc[:, testDescriptors.columns.isin(descriptors.columns.values)]\n",
    "\n",
    "prediction = results.predict(sm.add_constant(testDescriptors, has_constant='add'))\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission.loc[:, 'SalePrice'] = prediction\n",
    "sample_submission.to_csv('quiz1.csv', header=True, index=False)\n",
    "sample_submission.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}