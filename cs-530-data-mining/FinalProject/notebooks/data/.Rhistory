library(neuralnet)
for (i in c(1:length(sample_rules))) {
itests <-
str_split(as.character(sample_rules[i]), c("\\{", ",", "\\} => ", "\\}")) # will return clean series_test names
itests_score <- TestStu[, itests]
itests_score <- na.omit(itests_score)
samplesize <- dim(itests_score)[1]
print(paste('Total sample size in this model is', samplesize)) # check number of samples, make sure enough samples for neural network
# prepare data for neural network
set.seed(100)
index <- sample(samplesize, size = (0.8 * samplesize))
scaled_score <- apply(itests_score, 2, function(x) {
return(x / 100)
})
colnames(scaled_score) <-
make.names(itests) # remove space in name, otherwise may cause error for formula
train_data <- scaled_score[index, ]
test_data <- scaled_score[-index, ]
# fit neural network
dimension <- dim(scaled_score)[2]
f <- as.formula(paste0(
colnames(scaled_score)[dimension],
# last column is the score we want to predict
'~',
paste0(colnames(scaled_score)[-1], collapse = '+')
))
nn <- neuralnet(f,
data = train_data,
hidden = 2,
linear.output = F)
dev.new()
plot(nn)
# predict test
predict_nn <- compute(nn, test_data[, c(1:(dimension - 1))])
dev.new()
plot(
test_data[, dimension],
predict_nn$net.result,
pch = 1,
ylab = 'predicted percentage',
xlab = 'real percentage'
)
abline(0, 1, col = 'blue')
rmse <-
sqrt(sum((test_data[, dimension] - predict_nn$net.result) ** 2) / length(test_data))
print(paste('Root mean squre error is', rmse))
flush.console()
}
install.packages("arules")
install.packages("neuralnet")
install.packages("neuralnet")
print(TestStu_trans)
summary(rules)
inspect(rules[1:5])
rules_lift <- sort(rules, by = 'lift', decreasing = T)
inspect(head(rules_lift))
subset.mat <- is.subset(rules, rules)
subset.mat[lower.tri(subset.mat, diag = T)] <- NA
redundant <- colSums(subset.mat, na.rm = T) >= 1
redex <- which(redundant)
print(redex)
rules_sample <- rbind(as(rules[1], "data.frame"),
as(rules_lift[1], 'data.frame'))
rules_sample <- rbind(as(rules[1:3], "data.frame"),
as(rules_lift[1:3], 'data.frame'))
print(rules_sample)
setwd('C:/Users/Ryan/Desktop/DataAnalysis/data')
setwd('C:/Users/Ryan/Desktop/DataAnalysis/data')
#install.packages("reshape2","dplyr","plyr","corrplot")
td <-
read.csv('./TestSummaryAnalysis.csv',
stringsAsFactors = F,
header = T)
library(reshape2)
library(plyr)
library(dplyr)
##### data exploration
head(td)
td[td$PossibleCheating == 1, ]
td_noch <-
td[td$PossibleCheating == 0, ] # only work on no cheating records
# seems student may take same test more than one time, so will use their higest score as their performance evaluation for specific exam
Per_byt <-
td_noch %>% group_by(SeriesName, TestName, StudentId) %>% dplyr::summarise(MaxScorePercentage =
max(ScorePercentage)) # only keep highest score record for same exam same student
# try to figure student performance in different exams
Per_byt <- Per_byt[order(Per_byt$StudentId), ] # order by student Id
########### find student behavior pattern
Per_byt$Series_Test <- paste0(Per_byt$SeriesName, '_', Per_byt$TestName)
TestStu <-
dcast(Per_byt, StudentId ~ Series_Test, value.var = 'MaxScorePercentage')
dim(TestStu)
## convert to transaction record
TestStu_trans <- apply(TestStu[, 2:39], 2, function(x)
{
ifelse(is.na(x), FALSE, TRUE)
})
install.packages('arules')
library(arules)
td <- as(TestStu_trans, "transactions")
print(TestStu_trans)
rules <-
apriori(td, parameter = list(
supp = 0.003,
conf = 0.9,
minlen = 5
)) # limit minimum 5 classes taken by students
rules <- sort(rules, by = 'confidence', decreasing = TRUE)
summary(rules)
inspect(rules[1:5])
rules_lift <- sort(rules, by = 'lift', decreasing = T)
inspect(head(rules_lift))
subset.mat <- is.subset(rules, rules)
subset.mat[lower.tri(subset.mat, diag = T)] <- NA
redundant <- colSums(subset.mat, na.rm = T) >= 1
redex <- which(redundant)
# note, since much more students than number of tests, most rules are a subset of other rules
# pick out 2 popular rules with highest confidence and lift
rules_sample <- rbind(as(rules[1:3], "data.frame"),
as(rules_lift[1:3], 'data.frame'))
sample_rules <- rules_sample$rules
# find out students who took tests in above rules and build nn for each rule
str_split <- function(x, splits) {
for (split in splits) {
x <- unlist(strsplit(x, split))
}
return(x[!x == ""])
}
### use neural net to predict students' score based on their other tests' result
install.packages('neuralnet')
library(neuralnet)
for (i in c(1:length(sample_rules))) {
itests <-
str_split(as.character(sample_rules[i]), c("\\{", ",", "\\} => ", "\\}")) # will return clean series_test names
itests_score <- TestStu[, itests]
itests_score <- na.omit(itests_score)
samplesize <- dim(itests_score)[1]
print(paste('Total sample size in this model is', samplesize)) # check number of samples, make sure enough samples for neural network
# prepare data for neural network
set.seed(100)
index <- sample(samplesize, size = (0.8 * samplesize))
scaled_score <- apply(itests_score, 2, function(x) {
return(x / 100)
})
colnames(scaled_score) <-
make.names(itests) # remove space in name, otherwise may cause error for formula
train_data <- scaled_score[index, ]
test_data <- scaled_score[-index, ]
# fit neural network
dimension <- dim(scaled_score)[2]
f <- as.formula(paste0(
colnames(scaled_score)[dimension],
# last column is the score we want to predict
'~',
paste0(colnames(scaled_score)[-1], collapse = '+')
))
nn <- neuralnet(f,
data = train_data,
hidden = 2,
linear.output = F)
dev.new()
plot(nn)
# predict test
predict_nn <- compute(nn, test_data[, c(1:(dimension - 1))])
dev.new()
plot(
test_data[, dimension],
predict_nn$net.result,
pch = 1,
ylab = 'predicted percentage',
xlab = 'real percentage'
)
abline(0, 1, col = 'blue')
rmse <-
sqrt(sum((test_data[, dimension] - predict_nn$net.result) ** 2) / length(test_data))
print(paste('Root mean squre error is', rmse))
flush.console()
}
install.packages("arules")
install.packages("neuralnet")
install.packages("neuralnet")
print(sample_rules)
setwd('C:/Users/Ryan/Desktop/DataAnalysis/data')
#install.packages("reshape2","dplyr","plyr","corrplot")
td <-
read.csv('./TestSummaryAnalysis.csv',
stringsAsFactors = F,
header = T)
library(reshape2)
library(plyr)
library(dplyr)
##### data exploration
head(td)
td[td$PossibleCheating == 1, ]
td_noch <-
td[td$PossibleCheating == 0, ] # only work on no cheating records
# seems student may take same test more than one time, so will use their higest score as their performance evaluation for specific exam
Per_byt <-
td_noch %>% group_by(SeriesName, TestName, StudentId) %>% dplyr::summarise(MaxScorePercentage =
max(ScorePercentage)) # only keep highest score record for same exam same student
# try to figure student performance in different exams
Per_byt <- Per_byt[order(Per_byt$StudentId), ] # order by student Id
########### find student behavior pattern
Per_byt$Series_Test <- paste0(Per_byt$SeriesName, '_', Per_byt$TestName)
TestStu <-
dcast(Per_byt, StudentId ~ Series_Test, value.var = 'MaxScorePercentage')
dim(TestStu)
## convert to transaction record
TestStu_trans <- apply(TestStu[, 2:39], 2, function(x)
{
ifelse(is.na(x), FALSE, TRUE)
})
install.packages('arules')
library(arules)
td <- as(TestStu_trans, "transactions")
print(TestStu_trans)
rules <-
apriori(td, parameter = list(
supp = 0.003,
conf = 0.9,
minlen = 5
)) # limit minimum 5 classes taken by students
rules <- sort(rules, by = 'confidence', decreasing = TRUE)
summary(rules)
inspect(rules[1:5])
rules_lift <- sort(rules, by = 'lift', decreasing = T)
inspect(head(rules_lift))
subset.mat <- is.subset(rules, rules)
subset.mat[lower.tri(subset.mat, diag = T)] <- NA
redundant <- colSums(subset.mat, na.rm = T) >= 1
redex <- which(redundant)
# note, since much more students than number of tests, most rules are a subset of other rules
# pick out 2 popular rules with highest confidence and lift
rules_sample <- rbind(as(rules[1:3], "data.frame"),
as(rules_lift[1:3], 'data.frame'))
sample_rules <- rules_sample$rules
# find out students who took tests in above rules and build nn for each rule
str_split <- function(x, splits) {
for (split in splits) {
x <- unlist(strsplit(x, split))
}
return(x[!x == ""])
}
### use neural net to predict students' score based on their other tests' result
install.packages('neuralnet')
library(neuralnet)
for (i in c(1:length(sample_rules))) {
itests <-
str_split(as.character(sample_rules[i]), c("\\{", ",", "\\} => ", "\\}")) # will return clean series_test names
itests_score <- TestStu[, itests]
itests_score <- na.omit(itests_score)
samplesize <- dim(itests_score)[1]
print(paste('Total sample size in this model is', samplesize)) # check number of samples, make sure enough samples for neural network
# prepare data for neural network
set.seed(100)
index <- sample(samplesize, size = (0.8 * samplesize))
scaled_score <- apply(itests_score, 2, function(x) {
return(x / 100)
})
colnames(scaled_score) <-
make.names(itests) # remove space in name, otherwise may cause error for formula
train_data <- scaled_score[index, ]
test_data <- scaled_score[-index, ]
# fit neural network
dimension <- dim(scaled_score)[2]
f <- as.formula(paste0(
colnames(scaled_score)[dimension],
# last column is the score we want to predict
'~',
paste0(colnames(scaled_score)[-1], collapse = '+')
))
nn <- neuralnet(f,
data = train_data,
hidden = 2,
linear.output = F)
dev.new()
plot(nn)
# predict test
predict_nn <- compute(nn, test_data[, c(1:(dimension - 1))])
dev.new()
plot(
test_data[, dimension],
predict_nn$net.result,
pch = 1,
ylab = 'predicted percentage',
xlab = 'real percentage'
)
abline(0, 1, col = 'blue')
rmse <-
sqrt(sum((test_data[, dimension] - predict_nn$net.result) ** 2) / length(test_data))
print(paste('Root mean squre error is', rmse))
flush.console()
}
install.packages("arules")
install.packages("arules")
setwd('C:/Users/Ryan/Desktop/DataAnalysis/data')
td <-
read.csv('./TestSummaryAnalysis.csv',
stringsAsFactors = F,
header = T)
library(reshape2)
library(plyr)
library(dplyr)
##### data exploration
head(td)
td[td$PossibleCheating == 1, ]
td_noch <-
td[td$PossibleCheating == 0, ] # only work on no cheating records
# seems student may take same test more than one time, so will use their higest score as their performance evaluation for specific exam
Per_byt <-
td_noch %>% group_by(SeriesName, TestName, StudentId) %>% dplyr::summarise(MaxScorePercentage =
max(ScorePercentage)) # only keep highest score record for same exam same student
# try to figure student performance in different exams
Per_byt <- Per_byt[order(Per_byt$StudentId), ] # order by student Id
########### find student behavior pattern
Per_byt$Series_Test <- paste0(Per_byt$SeriesName, '_', Per_byt$TestName)
TestStu <-
dcast(Per_byt, StudentId ~ Series_Test, value.var = 'MaxScorePercentage')
dim(TestStu)
## convert to transaction record
TestStu_trans <- apply(TestStu[, 2:39], 2, function(x)
{
ifelse(is.na(x), FALSE, TRUE)
})
library(arules)
td <- as(TestStu_trans, "transactions")
print(TestStu_trans)
rules <-
apriori(td, parameter = list(
supp = 0.003,
conf = 0.9,
minlen = 5
)) # limit minimum 5 classes taken by students
rules <- sort(rules, by = 'confidence', decreasing = TRUE)
summary(rules)
inspect(rules[1:5])
rules_lift <- sort(rules, by = 'lift', decreasing = T)
inspect(head(rules_lift))
subset.mat <- is.subset(rules, rules)
subset.mat[lower.tri(subset.mat, diag = T)] <- NA
redundant <- colSums(subset.mat, na.rm = T) >= 1
redex <- which(redundant)
# note, since much more students than number of tests, most rules are a subset of other rules
# pick out 2 popular rules with highest confidence and lift
rules_sample <- rbind(as(rules[1], "data.frame"),
as(rules_lift[1], 'data.frame'))
sample_rules <- rules_sample$rules
# find out students who took tests in above rules and build nn for each rule
str_split <- function(x, splits) {
for (split in splits) {
x <- unlist(strsplit(x, split))
}
return(x[!x == ""])
}
### use neural net to predict students' score based on their other tests' result
library(neuralnet)
for (i in c(1:length(sample_rules))) {
itests <-
str_split(as.character(sample_rules[i]), c("\\{", ",", "\\} => ", "\\}")) # will return clean series_test names
itests_score <- TestStu[, itests]
itests_score <- na.omit(itests_score)
samplesize <- dim(itests_score)[1]
print(paste('Total sample size in this model is', samplesize)) # check number of samples, make sure enough samples for neural network
# prepare data for neural network
set.seed(100)
index <- sample(samplesize, size = (0.8 * samplesize))
scaled_score <- apply(itests_score, 2, function(x) {
return(x / 100)
})
colnames(scaled_score) <-
make.names(itests) # remove space in name, otherwise may cause error for formula
train_data <- scaled_score[index, ]
test_data <- scaled_score[-index, ]
# fit neural network
dimension <- dim(scaled_score)[2]
f <- as.formula(paste0(
colnames(scaled_score)[dimension],
# last column is the score we want to predict
'~',
paste0(colnames(scaled_score)[-1], collapse = '+')
))
nn <- neuralnet(f,
data = train_data,
hidden = 2,
linear.output = F)
dev.new()
plot(nn)
# predict test
predict_nn <- compute(nn, test_data[, c(1:(dimension - 1))])
dev.new()
plot(
test_data[, dimension],
predict_nn$net.result,
pch = 1,
ylab = 'predicted percentage',
xlab = 'real percentage'
)
abline(0, 1, col = 'blue')
rmse <-
sqrt(sum((test_data[, dimension] - predict_nn$net.result) ** 2) / length(test_data))
print(paste('Root mean squre error is', rmse))
flush.console()
}
itests_score <- TestStu[, itests]
setwd('C:/Users/Ryan/Desktop/DataAnalysis/data')
td <-
read.csv('./TestSummaryAnalysis.csv',
stringsAsFactors = F,
header = T)
library(reshape2)
library(plyr)
library(dplyr)
##### data exploration
head(td)
td[td$PossibleCheating == 1, ]
td_noch <-
td[td$PossibleCheating == 0, ] # only work on no cheating records
# seems student may take same test more than one time, so will use their higest score as their performance evaluation for specific exam
Per_byt <-
td_noch %>% group_by(SeriesName, TestName, StudentId) %>% dplyr::summarise(MaxScorePercentage =
max(ScorePercentage)) # only keep highest score record for same exam same student
# try to figure student performance in different exams
Per_byt <- Per_byt[order(Per_byt$StudentId), ] # order by student Id
########### find student behavior pattern
Per_byt$Series_Test <- paste0(Per_byt$SeriesName, '_', Per_byt$TestName)
TestStu <-
dcast(Per_byt, StudentId ~ Series_Test, value.var = 'MaxScorePercentage')
dim(TestStu)
## convert to transaction record
TestStu_trans <- apply(TestStu[, 2:39], 2, function(x)
{
ifelse(is.na(x), FALSE, TRUE)
})
library(arules)
td <- as(TestStu_trans, "transactions")
print(TestStu_trans)
rules <-
apriori(td, parameter = list(
supp = 0.003,
conf = 0.9,
minlen = 5
)) # limit minimum 5 classes taken by students
rules <- sort(rules, by = 'confidence', decreasing = TRUE)
summary(rules)
inspect(rules[1:5])
rules_lift <- sort(rules, by = 'lift', decreasing = T)
inspect(head(rules_lift))
subset.mat <- is.subset(rules, rules)
subset.mat[lower.tri(subset.mat, diag = T)] <- NA
redundant <- colSums(subset.mat, na.rm = T) >= 1
redex <- which(redundant)
# note, since much more students than number of tests, most rules are a subset of other rules
# pick out 2 popular rules with highest confidence and lift
rules_sample <- rbind(as(rules[1], "data.frame"),
as(rules_lift[1], 'data.frame'))
sample_rules <- rules_sample$rules
# find out students who took tests in above rules and build nn for each rule
str_split <- function(x, splits) {
for (split in splits) {
x <- unlist(strsplit(x, split))
}
return(x[!x == ""])
}
### use neural net to predict students' score based on their other tests' result
library(neuralnet)
for (i in c(1:length(sample_rules))) {
itests <-
str_split(as.character(sample_rules[i]), c("\\{", ",", "\\} => ", "\\}")) # will return clean series_test names
itests_score <- TestStu[, itests]
itests_score <- na.omit(itests_score)
samplesize <- dim(itests_score)[1]
print(paste('Total sample size in this model is', samplesize)) # check number of samples, make sure enough samples for neural network
# prepare data for neural network
set.seed(100)
index <- sample(samplesize, size = (0.8 * samplesize))
scaled_score <- apply(itests_score, 2, function(x) {
return(x / 100)
})
colnames(scaled_score) <-
make.names(itests) # remove space in name, otherwise may cause error for formula
train_data <- scaled_score[index, ]
test_data <- scaled_score[-index, ]
# fit neural network
dimension <- dim(scaled_score)[2]
f <- as.formula(paste0(
colnames(scaled_score)[dimension],
# last column is the score we want to predict
'~',
paste0(colnames(scaled_score)[-1], collapse = '+')
))
nn <- neuralnet(f,
data = train_data,
hidden = 2,
linear.output = F)
dev.new()
plot(nn)
# predict test
predict_nn <- compute(nn, test_data[, c(1:(dimension - 1))])
dev.new()
plot(
test_data[, dimension],
predict_nn$net.result,
pch = 1,
ylab = 'predicted percentage',
xlab = 'real percentage'
)
abline(0, 1, col = 'blue')
rmse <-
sqrt(sum((test_data[, dimension] - predict_nn$net.result) ** 2) / length(test_data))
print(paste('Root mean squre error is', rmse))
flush.console()
}
