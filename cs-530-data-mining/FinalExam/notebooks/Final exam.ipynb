{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 530 – Final take home exam  (20 points)\n",
    "## Due: Sunday, May 24 at 11:59am\n",
    "     ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "**This exam is individual and—unlike the final project—is therefore meant to be taken alone rather than in a group. Please write your name below to certify that you have worked on this exam by yourself and the code written in this exam as well as all the text explaining your results is yours and yours alone or code that you yourself have found only legitimately and in the public domain on the internet. You have not discussed this exam with anyone else and certainly have not used code, text, or other information from anyone in any part of the exam below. You have also not shared any code or text that you have written with anyone else.<br>\n",
    "Your name: _____________________________________________________ <br>**\n",
    "\n",
    "\n",
    "Make sure you submit your zipped file (Jupyter notebook + CSV file) to Canvas after you finish this exam (do not attach the dataset). In addition, we strongly encourage you to do all of the following to minimize loss of points of this exam:\n",
    "<li>   Double check and make sure that all your code blocks runs smoothly. This should be the very last thing you do before submitting. If you then change just one little thing in the code, check again that it all runs well before submitting. Submitting code that does not run will be highly detrimental for your grade.\n",
    "<li>   The Jupyter notebook should be readable, well designed and styled. It should look like a report to solve an important problem (not something put together hastily and haphazardly). Clean up your block outputs. Do not leave anything that might be confusing for the reader. In this exam, just like in real life, if your report is confusing to the reader, that is on you and not on the reader. One way to achieve this is to finish the exam well in advance of the deadline, go do something else for a day or two, and then read your submission carefully again. You will typically find that many things that were supposedly clear to you when you read the submission just after finishing writing it all no longer make sense when you read it a few days later. Naturally, if it is not clear to you a few days later, it would not be clear to any other reader too. This will give you a chance to modify your submission before finally submitting it.\n",
    "\n",
    "     \n",
    "     ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the questions below, make sure to use the appropriate goodness of fit measures in your discussion (R^2, adjusted R^2, AIC, BIC, accuracy, ROC AUC, ...). When you carry out any improvements you think you can make to the machine-learning models you build, you should explain whether they work, and—importantly—why. Visualizing the dataset and plotting various graphs as discussed below is important. But you must convince us that you understand why you got the results that you did**\n",
    "\n",
    "(8 points)  Q1. Load the dataset in the zip file with the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47520, 784) (6480, 784) (47520,) (6480,)\n",
      "(47520, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xtrain = np.loadtxt('data/xtrain.csv')\n",
    "xtest = np.loadtxt('data/xtest.csv')\n",
    "ytrain = np.loadtxt('data/ytrain.csv')\n",
    "ytest = np.loadtxt('data/ytest.csv')\n",
    "\n",
    "print(xtrain.shape, xtest.shape , ytrain.shape, ytest.shape )\n",
    "xtrain1 = xtrain.reshape((xtrain.shape[0], 28,28))\n",
    "print(xtrain1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of 47,520 training samples and 6,480 test samples. Each example is a 28x28 grayscale image, associated with a label from 1 of 9 possible classes. \n",
    "\n",
    "In this question, you will design a classification neural network. When asked to discuss various effects below, discussing them in relation to the bias-variance tradeoff is likely to be useful. \n",
    "\n",
    "<li> a) Visualize this dataset (5 samples for each class).<br>\n",
    "<li> b) Do the data preprocessing for your neural network. <br>\n",
    "<li> c)Use the Keras package and train a neural network to achieve at least 90% accuracy on the test set. Explain the architecture you chose—i.e., the number of layers, their width, the neuron types, the activation functions, etc. \n",
    "<li>d) Use at least 3 regularization techniques to improve the accuracy of the network you designed in part (a). Describe the improvement in the classification accuracy that you get from each technique. Explain why each technique worked or did not work.  <br>\n",
    "<li>e) Augment the training dataset (x2 and x3) and test your model. Which augmentation methods did you use? Explain why it did or did not improve the accuracy.  <br>\n",
    "\n",
    "**For part c, d, and e, plot the loss function and accuracy**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code for part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code for part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code for part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code for part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code for part e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8 points) Q2. Try at least 2 others classical (non-neural-network) machine-learning algorithms that we covered in this class to classify the same dataset. Make sure to tune any hyper-parameters of the models and plot the accuracy as a function of the values of those parameters. Explain those plots. What accuracies do you get? Why did the accuracies increase or decrease with relation to the values of the hyper-parameters? Discussing this with the bias-variance tradeoff in mind will likely be useful. How do these compare to the accuracies you got in Q1? Explain why the accuracies are higher, lower, or similar in terms of the assumptions, strengths, and weaknesses of the classification techniques you used in Q1 and Q2. Use the same data augmentation techniques that you used in Q1c with these algorithms. Compare the change in accuracy due to augmentation that you got here and that in Q1c. Explain any differences you might find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4 points) Q3. So far, we gave you both the training set and the test set for your models. But that is not what happens in real life. In real life, you prepare your model based on the training set to evaluate it on a test set to which you will have access only at some point in the future. To simulate that situation better, we are adding this question, which is related to evaluating your algorithms in Q1 and Q2 for predicting previously unseen data. On Friday, 5/22 at 12:01am, we will send you a collection of new samples without their associated labels. Your job is to run the algorithms you created for Q1 and Q2 on this new dataset and create a CSV file with 3 columns, which includes the labels that you predicted with your 3 proposed algorithms in the following order: first column is the NN from Q1, second column is the first model from Q2, and third column is the second model from Q2. You need to upload this CSV file to Canvas as your answer to this question. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}